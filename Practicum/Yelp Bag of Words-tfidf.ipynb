{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model on Amazon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from Word2VecUtility3 import Word2VecUtility3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Yelp_Evenly_Sampled164k.csv', sep=',', index_col=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick look at the reviews:\n",
      "   stars                                               text\n",
      "0      1  I just got my reading glasses back and have no...\n",
      "1      1  Absolutely the \"Best of Phoenix\". Caring compa...\n",
      "2      1  I've been coming here for about 4 years now. I...\n",
      "3      1  The food was hot and tasty. The garlic knots a...\n",
      "4      1  This place is delicious!! I love the salmon an...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    82000\n",
       "0    82000\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('A quick look at the reviews:')\n",
    "print(data.head())\n",
    "data['stars'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split dataset into train/test sets\n",
    "train_data = data.sample(frac=0.7,random_state=200)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "train_data.to_csv('train_Even82k.csv', index=False, sep=',', encoding='utf-8')\n",
    "test_data.to_csv('test_Even82k.csv', index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text\n",
      "0      1  This store has the nIcest most knowledgeable p...\n",
      "1      0  I am a barbeque EXPERT and I have to say...thi...\n",
      "   stars                                               text\n",
      "0      1  I just got my reading glasses back and have no...\n",
      "1      1  I've been coming here for about 4 years now. I...\n"
     ]
    }
   ],
   "source": [
    "#load train/test sets\n",
    "train = pd.read_csv('train_Even82k.csv', index_col=False)\n",
    "test = pd.read_csv('test_Even82k.csv', index_col=False)\n",
    "\n",
    "# print (\"The number of training samples are: %r\") % (len(train))\n",
    "# print (\"The number of testing samples are: %r \\n\") % (len(test))\n",
    "\n",
    "#make sure the train/test tests are formatted correctly.\n",
    "print(train.iloc[:2])\n",
    "print(test.iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing & Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set reviews...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.azcentral.com/news/articles/2010/12/20/20101220chandler-movers-sued-judgment.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "\n",
    "for i in range( 0, len(train[\"text\"])):\n",
    "    clean_train_reviews.append(\" \".join(Word2VecUtility3.review_to_wordlist(train[\"text\"][i], True)))\n",
    "\n",
    "clean_test_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set reviews...\\n\")\n",
    "for i in range(0,len(test[\"text\"])):\n",
    "    clean_test_reviews.append(\" \".join(Word2VecUtility3.review_to_wordlist(test[\"text\"][i], True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,stop_words=None,max_features=5000)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Tuning with GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRID SEARCH:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2', 'elasticnet'], 'loss': ('log', 'hinge'), 'alpha': [0.001, 0.0001, 1e-05, 1e-06]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'loss': ('log', 'hinge'),\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha': [0.001, 0.0001, 0.00001, 0.000001]\n",
    "}\n",
    "\n",
    "print()\n",
    "print(\"GRID SEARCH:\")\n",
    "grid_search = GridSearchCV(SGDClassifier(), parameters, cv=3)\n",
    "grid_search.fit(train_data_features, train['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for data: 0.928527874564\n",
      "Best loss: log\n",
      "Best penalty: elasticnet\n",
      "Best alpha: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# View the accuracy score\n",
    "print('Best score for data:', grid_search.best_score_)\n",
    "# View the best parameters for the model found using grid search\n",
    "print('Best loss:', grid_search.best_estimator_.loss)\n",
    "print('Best penalty:', grid_search.best_estimator_.penalty)\n",
    "print('Best alpha:', grid_search.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=1e-05, max_iter=5, random_state=42)\n",
    "SVM = SVM.fit( train_data_features, train[\"stars\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93158536585365859"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the classifier trained using training data to test set, and view the accuracy score\n",
    "SVM.score(test_data_features, test['stars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(train_data_features, train[\"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88491869918699184"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_data_features, test['stars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try SVM BoW with Radomly Distributed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Yelp_Randomly_Distributed_164k.csv', sep=',', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split dataset into train/test sets\n",
    "#changed names so that we don't contaminate data \n",
    "train_data = df.sample(frac=0.7,random_state=200)\n",
    "test_data = df.drop(train_data.index)\n",
    "\n",
    "train_data.to_csv('train_Random.csv', index=False, sep=',', encoding='utf-8')\n",
    "test_data.to_csv('test_Random.csv', index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples are: 114800\n",
      "The number of testing samples are: 49200 \n",
      "\n",
      "   stars                                               text\n",
      "0      1  As the owner of a small business, I know just ...\n",
      "1      1  They trying to kill me here - downtown is alre...\n",
      "   stars                                               text\n",
      "0      1  When it comes to Indian food this place is alw...\n",
      "1      1  I think this shop is probably more of a 3.5, b...\n"
     ]
    }
   ],
   "source": [
    "#load train/test sets\n",
    "train1 = pd.read_csv('train_Random.csv', index_col=False)\n",
    "test1 = pd.read_csv('test_Random.csv', index_col=False)\n",
    "\n",
    "print((\"The number of training samples are: %r\") % (len(train1)))\n",
    "print((\"The number of testing samples are: %r \\n\") % (len(test1)))\n",
    "\n",
    "#make sure the train/test tests are formatted correctly.\n",
    "print(train1.iloc[:2])\n",
    "print(test1.iloc[:2])\n",
    "#print(train1['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the Amazon reviews...\n",
      "\n",
      "Cleaning and parsing the test set reviews...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews1 = []\n",
    "\n",
    "print(\"Cleaning and parsing the Amazon reviews...\\n\")\n",
    "for i in range( 0, len(train1[\"text\"])):\n",
    "    clean_train_reviews1.append(\" \".join(Word2VecUtility3.review_to_wordlist(train1[\"text\"][i], True)))\n",
    "\n",
    "clean_test_reviews1 = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set reviews...\\n\")\n",
    "for i in range(0,len(test1[\"text\"])):\n",
    "    clean_test_reviews1.append(\" \".join(Word2VecUtility3.review_to_wordlist(test1[\"text\"][i], True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,stop_words=None,max_features=5000)\n",
    "\n",
    "train_data_features1 = vectorizer.fit_transform(clean_train_reviews1)\n",
    "train_data_features1 = train_data_features1.toarray()\n",
    "\n",
    "test_data_features1 = vectorizer.transform(clean_test_reviews1)\n",
    "test_data_features1 = test_data_features1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRID SEARCH:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2', 'elasticnet'], 'loss': ('log', 'hinge'), 'alpha': [0.001, 0.0001, 1e-05, 1e-06]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'loss': ('log', 'hinge'),\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha': [0.001, 0.0001, 0.00001, 0.000001]\n",
    "}\n",
    "\n",
    "print()\n",
    "print(\"GRID SEARCH:\")\n",
    "grid_search1 = GridSearchCV(SGDClassifier(), parameters, cv=3)\n",
    "grid_search1.fit(train_data_features1, train1['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for data: 0.940165505226\n",
      "Best loss: log\n",
      "Best penalty: elasticnet\n",
      "Best alpha: 1e-05\n"
     ]
    }
   ],
   "source": [
    "# View the accuracy score\n",
    "print('Best score for data:', grid_search1.best_score_)\n",
    "# View the best parameters for the model found using grid search\n",
    "print('Best loss:', grid_search1.best_estimator_.loss)\n",
    "print('Best penalty:', grid_search1.best_estimator_.penalty)\n",
    "print('Best alpha:', grid_search1.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM1 = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=1e-05, max_iter=5, random_state=42)\n",
    "SVM1 = SVM1.fit( train_data_features1, train1[\"stars\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92871951219512194"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the classifier trained using training data to test set, and view the accuracy score\n",
    "SVM1.score(test_data_features1, test1['stars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(train_data_features1, train1[\"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89762195121951216"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_data_features1, test1['stars'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
