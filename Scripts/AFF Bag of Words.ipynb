{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model on Amazon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook carries out three tests:\n",
    "\n",
    "1: BoW using an SVM on an unbalanced dataset: Result: 0.92 (Bigram 0.81)\n",
    "\n",
    "2: BoW using Multinomial NB and tf idf: Result: 0.85\n",
    "\n",
    "3: Bow using an SVM on a balanced dataset: Result: 0.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from Word2VecUtility import Word2VecUtility\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reviews.csv', sep=',', index_col=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick look at the reviews:\n",
      "   Score                                               Text\n",
      "0      5  I have bought several of the Vitality canned d...\n",
      "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
      "2      4  This is a confection that has been around a fe...\n",
      "3      2  If you are looking for the secret ingredient i...\n",
      "4      5  Great taffy at a great price.  There was a wid...\n"
     ]
    }
   ],
   "source": [
    "print 'A quick look at the reviews:'\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 86986, 223984, 365727, 109874, 424481, 426869,  21208, 369621,\n",
      "            325219, 209866,\n",
      "            ...\n",
      "            545834, 226170, 498791, 219611,  92919,  21690, 350418, 561044,\n",
      "            278642, 220337],\n",
      "           dtype='int64', length=100000)\n"
     ]
    }
   ],
   "source": [
    "#Create a sample dataset to speed up training times for the moment. \n",
    "size = 100000 \n",
    "subdata = data.sample(n = size, random_state=520)\n",
    "\n",
    "#subdata = subdata[pd.notnull(subdata['text'])] - to get rid of null values\n",
    "print subdata.index\n",
    "subdata.to_csv('review_sub100k.csv', index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(data)\n",
    "data = subdata\n",
    "del(subdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score                                               Text\n",
      "0      2  Greenies tries to position itself as a healthy...\n",
      "1      5  Love the flavor!  Not too strong, but not weak...\n",
      "2      1  I can't comment on the other flavors of Silk s...\n",
      "3      4  I've enjoyed the Dark Magic coffee and re-orde...\n",
      "4      5  I first found Primo Pasta at a local restauran...\n"
     ]
    }
   ],
   "source": [
    "#Load in the sample data\n",
    "data = pd.read_csv('review_sub100k.csv', index_col=False)\n",
    "print data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score                                               Text\n",
      "0      2  Greenies tries to position itself as a healthy...\n",
      "1      5  Love the flavor!  Not too strong, but not weak...\n",
      "2      1  I can't comment on the other flavors of Silk s...\n",
      "3      4  I've enjoyed the Dark Magic coffee and re-orde...\n",
      "4      5  I first found Primo Pasta at a local restauran...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5    63770\n",
       "4    14133\n",
       "1     9219\n",
       "2     5280\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove rows which contain ratings of 3 (Neutral and not included in our analysis.)\n",
    "data = data[data.Score != 3]\n",
    "print(data.head())\n",
    "data['Score'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    77903\n",
       "0    14499\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.Score <=2, 'Score'] = 0\n",
    "data.loc[data.Score >=4, 'Score'] = 1\n",
    "        \n",
    "data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score                                               Text\n",
      "0      0  Greenies tries to position itself as a healthy...\n",
      "1      1  Love the flavor!  Not too strong, but not weak...\n",
      "2      0  I can't comment on the other flavors of Silk s...\n",
      "3      1  I've enjoyed the Dark Magic coffee and re-orde...\n",
      "4      1  I first found Primo Pasta at a local restauran...\n"
     ]
    }
   ],
   "source": [
    "#make sure the reviews were labelled correctly (can compare to the previous header)\n",
    "print data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#train, test = train_test_split(data, test_size = 0.3)\n",
    "\n",
    "#split dataset into train/test sets\n",
    "train_data = data.sample(frac=0.7,random_state=200)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "train_data.to_csv('train.csv', index=False, sep=',', encoding='utf-8')\n",
    "test_data.to_csv('test.csv', index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples are: 64681\n",
      "The number of testing samples are: 27721 \n",
      "\n",
      "   Score                                               Text\n",
      "0      1  Honey Maid Graham Cracker Crust are the best c...\n",
      "1      1  I have only been eating more healthy foods for...\n",
      "   Score                                               Text\n",
      "0      0  Greenies tries to position itself as a healthy...\n",
      "1      1  I've enjoyed the Dark Magic coffee and re-orde...\n"
     ]
    }
   ],
   "source": [
    "#load train/test sets\n",
    "train = pd.read_csv('train.csv', index_col=False)\n",
    "test = pd.read_csv('test.csv', index_col=False)\n",
    "\n",
    "print (\"The number of training samples are: %r\") % (len(train))\n",
    "print (\"The number of testing samples are: %r \\n\") % (len(test))\n",
    "\n",
    "#make sure the train/test tests are formatted correctly.\n",
    "print train.iloc[:2]\n",
    "print test.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing & Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the Amazon reviews...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\Anaconda3\\envs\\python2\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Users\\James\\Anaconda3\\envs\\python2\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list. Word2VecUtility is a text processing function imported from another file. \n",
    "\n",
    "print(\"Cleaning and parsing the Amazon reviews...\\n\")\n",
    "for i in range( 0, len(train[\"Text\"])):\n",
    "    clean_train_reviews.append(\" \".join(Word2VecUtility.review_to_wordlist(train[\"Text\"][i], True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ****** Create a bag of words from the training set\n",
    "#\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                         tokenizer = None,    \\\n",
    "                         preprocessor = None, \\\n",
    "                         stop_words = None,   \\\n",
    "                         max_features = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SVM (this may take a while)...\n"
     ]
    }
   ],
   "source": [
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of\n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an\n",
    "# array\n",
    "np.asarray(train_data_features)\n",
    "\n",
    "# ******* Train an SVM using the bag of words\n",
    "#\n",
    "print(\"Training the SVM (this may take a while)...\")\n",
    "\n",
    "# Fit the SVM to the training set, using the bag of words as\n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# Initialize an SVM classifier with chosen parameters.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SVM = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "SVM = SVM.fit( train_data_features, train[\"Score\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Predicting test labels...\n",
      "\n",
      "Wrote results to Bag_of_Words_modelSVM100k.csv\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list and append the clean reviews one by one\n",
    "clean_test_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,len(test[\"Text\"])):\n",
    "    clean_test_reviews.append(\" \".join(Word2VecUtility.review_to_wordlist(test[\"Text\"][i], True)))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "np.asarray(test_data_features)\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "print(\"Predicting test labels...\\n\")\n",
    "result = SVM.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"Score\"], \"Sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join('Bag_of_Words_modelSVM100k.csv'), index=False, quoting=3)\n",
    "print(\"Wrote results to Bag_of_Words_modelSVM100k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915731755709\n"
     ]
    }
   ],
   "source": [
    "accuraccy = np.mean(result == test['Score'])  \n",
    "\n",
    "print(accuraccy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ****** Create a bag of words from the training set n GRAM\n",
    "#\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizerb = CountVectorizer(analyzer = \"word\",   \\\n",
    "                         ngram_range=(2,2),     \n",
    "                         tokenizer = None,    \\\n",
    "                         preprocessor = None, \\\n",
    "                         stop_words = None,   \\\n",
    "                         max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SVM (this may take a while)...\n"
     ]
    }
   ],
   "source": [
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of\n",
    "# strings.\n",
    "train_data_features = vectorizerb.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an\n",
    "# array\n",
    "np.asarray(train_data_features)\n",
    "\n",
    "# ******* Train an SVM using the bag of words\n",
    "#\n",
    "print(\"Training the SVM (this may take a while)...\")\n",
    "\n",
    "# Fit the SVM to the training set, using the bag of words as\n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# Initialize an SVM classifier with chosen parameters.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SVM = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "SVM = SVM.fit( train_data_features, train[\"Score\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Predicting test labels...\n",
      "\n",
      "Wrote results to Bag_of_Words_modelSVMbigram100k.csv\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list and append the clean reviews one by one\n",
    "clean_test_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,len(test[\"Text\"])):\n",
    "    clean_test_reviews.append(\" \".join(Word2VecUtility.review_to_wordlist(test[\"Text\"][i], True)))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "np.asarray(test_data_features)\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "print(\"Predicting test labels...\\n\")\n",
    "result = SVM.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"Score\"], \"Sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join('Bag_of_Words_modelSVMbigram100k.csv'), index=False, quoting=3)\n",
    "print(\"Wrote results to Bag_of_Words_modelSVMbigram100k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814184192489\n"
     ]
    }
   ],
   "source": [
    "accuraccy = np.mean(result == test['Score'])  \n",
    "\n",
    "print(accuraccy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trying some other BOW models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(clean_train_reviews)\n",
    "train_counts.shape\n",
    "(2257, 35788)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(train_counts)\n",
    "train_tf = tf_transformer.transform(train_counts)\n",
    "train_tf.shape\n",
    "(2257, 35788)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "train_tfidf.shape\n",
    "(2257, 35788)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(train_tfidf, train[\"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use Testing Data \n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "clean_test_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,len(test[\"Text\"])):\n",
    "    clean_test_reviews.append(\" \".join(Word2VecUtility.review_to_wordlist(test[\"Text\"][i], True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.847840986977\n"
     ]
    }
   ],
   "source": [
    "new_counts = count_vect.transform(clean_test_reviews)\n",
    "new_tfidf = tfidf_transformer.transform(new_counts)\n",
    "\n",
    "result = clf.predict(new_tfidf)\n",
    "a = np.mean(result == test['Score'])  \n",
    "print(a)\n",
    "\n",
    "\n",
    "output = pd.DataFrame( data={\"Score\":test[\"Score\"], \"Sentiment\":result} )\n",
    "\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"Bag_of_Words_model100kSVMTFIDF.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try SVM BoW with Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('AFF_Evenly_Sampled.csv', sep=',', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score                                               Text\n",
      "0      1  I like this brand. I didn't realize I was orde...\n",
      "1      1  Being my wife is a licensed cosmetologist and ...\n",
      "2      1  If you are looking for an upgrade from the sta...\n",
      "3      1  I am so allergic to too many artificial sweete...\n",
      "4      1  I have not been able to find this locally and ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    82000\n",
       "0    82000\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure the data is evenly distributed\n",
    "print df.iloc[:5]\n",
    "df['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the sampled data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split dataset into train/test sets\n",
    "#changed names so that we don't contaminate data \n",
    "train_balanced_data = df.sample(frac=0.7,random_state=200)\n",
    "test_balanced_data = df.drop(train_balanced_data.index)\n",
    "\n",
    "train_balanced_data.to_csv('train_balanced.csv', index=False, sep=',', encoding='utf-8')\n",
    "test_balanced_data.to_csv('test_balanced.csv', index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples are: 114800\n",
      "The number of testing samples are: 49200 \n",
      "\n",
      "   Score                                               Text\n",
      "0      1  I had been frustrated trying to figure out how...\n",
      "1      0  I tried this product in hopes that it would in...\n",
      "   Score                                               Text\n",
      "0      1  I like this brand. I didn't realize I was orde...\n",
      "1      1  If you are looking for an upgrade from the sta...\n"
     ]
    }
   ],
   "source": [
    "#load train/test sets\n",
    "train1 = pd.read_csv('train_balanced.csv', index_col=False)\n",
    "test1 = pd.read_csv('test_balanced.csv', index_col=False)\n",
    "\n",
    "print (\"The number of training samples are: %r\") % (len(train1))\n",
    "print (\"The number of testing samples are: %r \\n\") % (len(test1))\n",
    "\n",
    "#make sure the train/test tests are formatted correctly.\n",
    "print train1.iloc[:2]\n",
    "print test1.iloc[:2]\n",
    "#print(train1['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the Amazon reviews...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews1 = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list. Word2VecUtility is a text processing function imported from another file. \n",
    "\n",
    "print(\"Cleaning and parsing the Amazon reviews...\\n\")\n",
    "for i in range( 0, len(train1[\"Text\"])):\n",
    "    clean_train_reviews1.append(\" \".join(Word2VecUtility.review_to_wordlist(train1[\"Text\"][i], True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ****** Create a bag of words from the training set\n",
    "#\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer1 = CountVectorizer(analyzer = \"word\",   \\\n",
    "                         tokenizer = None,    \\\n",
    "                         preprocessor = None, \\\n",
    "                         stop_words = None,   \\\n",
    "                         max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SVM (this may take a while)...\n"
     ]
    }
   ],
   "source": [
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of\n",
    "# strings.\n",
    "train_data_features1 = vectorizer1.fit_transform(clean_train_reviews1)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an\n",
    "# array\n",
    "np.asarray(train_data_features1)\n",
    "\n",
    "# ******* Train an SVM using the bag of words\n",
    "#\n",
    "print(\"Training the SVM (this may take a while)...\")\n",
    "\n",
    "# Fit the SVM to the training set, using the bag of words as\n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# Initialize an SVM classifier with chosen parameters.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SVM1 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "SVM1 = SVM1.fit( train_data_features1, train1[\"Score\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Predicting test labels...\n",
      "\n",
      "Wrote results to Bag_of_Words_model.csv\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list and append the clean reviews one by one\n",
    "clean_test_reviews1 = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,len(test1[\"Text\"])):\n",
    "    clean_test_reviews1.append(\" \".join(Word2VecUtility.review_to_wordlist(test1[\"Text\"][i], True)))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features1 = vectorizer1.transform(clean_test_reviews1)\n",
    "np.asarray(test_data_features1)\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "print(\"Predicting test labels...\\n\")\n",
    "result1 = SVM1.predict(test_data_features1)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output1 = pd.DataFrame( data={\"id\":test1[\"Score\"], \"Sentiment\":result1} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output1.to_csv(os.path.join('Bag_of_Words_Model_BalancedSVM82k.csv'), index=False, quoting=3)\n",
    "print(\"Wrote results to Bag_of_Words_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.884552845528\n"
     ]
    }
   ],
   "source": [
    "accuraccy1 = np.mean(result1 == test1['Score'])  \n",
    "\n",
    "print(accuraccy1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
