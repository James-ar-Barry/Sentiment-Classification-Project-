{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Which uses Keras' embedding layer\n",
    "\n",
    "Accuraccy: 70% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from Word2VecUtility3 import Word2VecUtility3\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.6\n",
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "print(keras.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_volcabulary_and_list_words(data):\n",
    "    reviews_words = []\n",
    "    volcabulary = []\n",
    "    for review in data[\"text\"]:\n",
    "        review_words = Word2VecUtility3.review_to_wordlist(\n",
    "            review, remove_stopwords=True)\n",
    "        reviews_words.append(review_words)\n",
    "        for word in review_words:\n",
    "            volcabulary.append(word)\n",
    "    volcabulary = set(volcabulary)\n",
    "    return volcabulary, reviews_words\n",
    "\n",
    "def get_reviews_word_index(reviews_words, volcabulary, max_words, max_length):\n",
    "    word2index = {word: i for i, word in enumerate(volcabulary)}\n",
    "    # use w in volcabulary to limit index within max_words\n",
    "    reviews_words_index = [[start] + [(word2index[w] + index_from) for w in review] for review in reviews_words]\n",
    "    # in word2vec embedding, use (i < max_words + index_from) because we need the exact index for each word, in order to map it to its vector. And then its max_words is 5003 instead of 5000.\n",
    "    reviews_words_index = [[i if (i < max_words) else oov for i in index] for index in reviews_words_index]\n",
    "    # padding with 0, each review has max_length now.\n",
    "    reviews_words_index = sequence.pad_sequences(reviews_words_index, maxlen=max_length, padding='post', truncating='post')\n",
    "    return reviews_words_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get volcabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get reviews_words_index...\n",
      "[[    1     2  9094     2     2     2     2 10594 12437     2     2     2]\n",
      " [    1     2     2     2     2     2     2     2     2 10146     2 17723]\n",
      " [    1     2     2     2 16399     2     2     2 18239     2     2  3059]\n",
      " [    1 11518     2 11616     2     2  6832     2     2 12902     2  4242]\n",
      " [    1     2     2  8753     2 18513     2 17029  8935     2   615     2]\n",
      " [    1     2     2     2     2     2     2     2  7219     2     2     2]\n",
      " [    1     2     2  2161     2     2     2 11518 18579     2     2 16399]\n",
      " [    1     2 11597 14090     2     2     2     2     2     2     2     2]\n",
      " [    1     2     2     2     2     2     2     2     2     2  3106  1426]\n",
      " [    1     2     2     2 15410  4242     2     2     2     2     2     2]\n",
      " [    1 11518     2  8912     2     2  1975     2     2     2     2  9572]\n",
      " [    1  3866     2 11463     2     2     2     2  3866     2 18513     2]\n",
      " [    1     2     2     2 16522     2     2     2     2  7460 18663     2]\n",
      " [    1  9572     2     2     2     2  8151     2 18579     2     2     2]\n",
      " [    1     2 11518     2     2  7634     2     2     2  2805 16070     2]\n",
      " [    1     2  7460     2     2     2     2  9629     2     2  7460  3362]\n",
      " [    1     2     2     2     2 19660     2     2     2     2     2     2]\n",
      " [    1     2   795     2     2     2     2     2 17254     2     2     2]\n",
      " [    1     2     2     2     2     2     2     2     2     2  2517     2]\n",
      " [    1     2     2     2 18319     2     2     2  2616 19660     2     2]]\n",
      "(99999, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79999, 500)\n",
      "(20000, 500)\n"
     ]
    }
   ],
   "source": [
    "# data processing para\n",
    "max_words = 20000\n",
    "max_length = 500\n",
    "\n",
    "# model training parameters\n",
    "batch_size = 32\n",
    "embedding_dims = 100 #was 100\n",
    "# nb_filter = 250\n",
    "# filter_length = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "# index trick parameters\n",
    "index_from = 3\n",
    "start = 1\n",
    "# padding = 0\n",
    "oov = 2\n",
    "\n",
    "data = pd.read_csv('yelp_review_sub100k.csv', sep=',', index_col=False)\n",
    "print('get volcabulary...')\n",
    "volcabulary, reviews_words = get_volcabulary_and_list_words(data)\n",
    "print('get reviews_words_index...')\n",
    "reviews_words_index = get_reviews_word_index(reviews_words, volcabulary, max_words, max_length)\n",
    "\n",
    "print(reviews_words_index[:20, :12])\n",
    "print(reviews_words_index.shape)\n",
    "\n",
    "labels = data[\"stars\"]\n",
    "labels[labels <= 2] = 0\n",
    "labels[labels >= 4] = 1\n",
    "\n",
    "pickle.dump((reviews_words_index, labels), open(\"100000by500reviews_words_index.pkl\", 'wb'))\n",
    "\n",
    "# with oov, index_from, start and padding, we have 4999 + 4 = 5003 indexes.\n",
    "(reviews_words_index, labels) = pickle.load(open(\"100000by500reviews_words_index.pkl\", 'rb'))\n",
    "\n",
    "index = np.arange(reviews_words_index.shape[0])\n",
    "train_index, valid_index = train_test_split(\n",
    "    index, train_size=0.8, random_state=520)\n",
    "\n",
    "train_data = reviews_words_index[train_index]\n",
    "valid_data = reviews_words_index[valid_index]\n",
    "train_labels = labels[train_index]\n",
    "valid_labels = labels[valid_index]\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "\n",
    "del(labels, train_index, valid_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79999 train sequences\n",
      "20000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), 'train sequences')\n",
    "print(len(valid_labels), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #might not be necessary \n",
    "# print('Pad sequences (samples x time)')\n",
    "# train_data = sequence.pad_sequences(train_data, maxlen=max_length)\n",
    "# valid_data = sequence.pad_sequences(valid_data, maxlen=max_length)\n",
    "# print('train_data shape:', train_data.shape)\n",
    "# print('valid_data:', valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          2000300   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               12500250  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 14,500,801\n",
      "Trainable params: 14,500,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "#model.add(Embedding(max_words, 32, input_length=max_length))\n",
    "model.add(Embedding(max_words + index_from, embedding_dims, \\\n",
    "                    input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79999 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "16s - loss: -7.4220e-01 - acc: 0.6611 - val_loss: -1.5056e+00 - val_acc: 0.6846\n",
      "Epoch 2/10\n",
      "15s - loss: -1.6613e+00 - acc: 0.6923 - val_loss: -1.5732e+00 - val_acc: 0.6859\n",
      "Epoch 3/10\n",
      "15s - loss: -1.9765e+00 - acc: 0.7102 - val_loss: -1.6478e+00 - val_acc: 0.6967\n",
      "Epoch 4/10\n",
      "15s - loss: -2.2641e+00 - acc: 0.7271 - val_loss: -1.4945e+00 - val_acc: 0.7058\n",
      "Epoch 5/10\n",
      "15s - loss: -2.5923e+00 - acc: 0.7488 - val_loss: -1.5496e+00 - val_acc: 0.7045\n",
      "Epoch 6/10\n",
      "15s - loss: -2.8325e+00 - acc: 0.7679 - val_loss: -1.4926e+00 - val_acc: 0.6962\n",
      "Epoch 7/10\n",
      "14s - loss: -3.0209e+00 - acc: 0.7854 - val_loss: -1.4313e+00 - val_acc: 0.7005\n",
      "Epoch 8/10\n",
      "14s - loss: -3.1441e+00 - acc: 0.7977 - val_loss: -1.2266e+00 - val_acc: 0.7015\n",
      "Epoch 9/10\n",
      "15s - loss: -3.2176e+00 - acc: 0.8086 - val_loss: -1.2207e+00 - val_acc: 0.7024\n",
      "Epoch 10/10\n",
      "14s - loss: -3.2670e+00 - acc: 0.8153 - val_loss: -1.1059e+00 - val_acc: 0.7029\n",
      "Accuracy: 70.29%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(train_data, train_labels, validation_data=(valid_data, valid_labels), epochs=10, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(valid_data, valid_labels, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# filepath=\"imp-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# csv_logger = CSVLogger('training_history.csv')\n",
    "#Accuracy: 70.83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"start training model...\")\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(max_words + index_from, embedding_dims, \\\n",
    "#                     input_length=max_length))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# # we add a Convolution1D, which will learn nb_filter\n",
    "# # word group filters of size filter_length:\n",
    "\n",
    "# # filter_length is like filter size, subsample_length is like step in 2D CNN.\n",
    "# model.add(Convolution1D(nb_filter=nb_filter,\n",
    "#                         filter_length=filter_length,\n",
    "#                         border_mode='valid',\n",
    "#                         activation='relu',\n",
    "#                         subsample_length=1))\n",
    "# # we use standard max pooling (halving the output of the previous layer):\n",
    "# model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "# # We flatten the output of the conv layer,\n",
    "# # so that we can add a vanilla dense layer:\n",
    "# model.add(Flatten())\n",
    "\n",
    "# # We add a vanilla hidden layer:\n",
    "# model.add(Dense(hidden_dims))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               class_mode='binary',\n",
    "#              metrics=['accuracy'])\n",
    "# model.fit(train_data, train_labels, validation_data=(valid_data, valid_labels), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
