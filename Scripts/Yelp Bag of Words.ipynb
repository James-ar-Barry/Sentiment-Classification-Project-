{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model on Amazon Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from Word2VecUtility import Word2VecUtility\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick look at the reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>If you enjoy service by someone who is as comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>After being on the phone with Verizon Wireless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Great service! Corey is very service oriented....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended. Went in yesterday looking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I walked in here looking for a specific piece ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text\n",
       "0      5  If you enjoy service by someone who is as comp...\n",
       "1      5  After being on the phone with Verizon Wireless...\n",
       "2      5  Great service! Corey is very service oriented....\n",
       "3      5  Highly recommended. Went in yesterday looking ...\n",
       "4      4  I walked in here looking for a specific piece ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Yelp_reviews.csv', sep=',', index_col=False)\n",
    "\n",
    "print 'A quick look at the reviews:'\n",
    "data.head()\n",
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "If you enjoy service by someone who is as competent as he is personable, I would recommend Corey Kaplan highly. The time he has spent here has been very productive and working with him educational and enjoyable. I hope not to need him again (though this is highly unlikely) but knowing he is there if I do is very nice. By the way, I'm not from El Centro, CA. but Scottsdale, AZ.\n"
     ]
    }
   ],
   "source": [
    "print data.stars[0]\n",
    "print data.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([2807975, 1755175, 1813426, 2712021,  235453, 2221447, 2599015,\n",
      "            1501646, 1623687, 3236951,\n",
      "            ...\n",
      "            2213442, 2187233, 3756712, 3637900,  343447,  556202, 3295730,\n",
      "            2160073, 1753741, 2807019],\n",
      "           dtype='int64', length=99999)\n"
     ]
    }
   ],
   "source": [
    "#Create a sample dataset to speed up training times for the moment. \n",
    "size = 100000 \n",
    "subdata = data.sample(n = size, random_state=520)\n",
    "subdata = subdata[pd.notnull(subdata['text'])] # to get rid of null values\n",
    "print subdata.index\n",
    "subdata.to_csv('yelp_review_sub100k.csv', index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(data)\n",
    "data = subdata\n",
    "del(subdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text\n",
      "0      3  I don't visit dollar stores often and when I d...\n",
      "1      2  The food was perfectly adequate, nothing speci...\n",
      "2      2  Our server, Cookie, has been fabulous both tim...\n",
      "3      1  DO NOT go here, I made reservations for a holi...\n",
      "4      5  My husband and I stopped in for a foot rub and...\n"
     ]
    }
   ],
   "source": [
    "#Load in the sample data\n",
    "data = pd.read_csv('yelp_review_sub100k.csv', index_col=False)\n",
    "print data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text\n",
      "1      2  The food was perfectly adequate, nothing speci...\n",
      "2      2  Our server, Cookie, has been fabulous both tim...\n",
      "3      1  DO NOT go here, I made reservations for a holi...\n",
      "4      5  My husband and I stopped in for a foot rub and...\n",
      "6      5  Always great, fresh food. Quick service. \\n\\nT...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5    40839\n",
       "4    24767\n",
       "1    13162\n",
       "2     8744\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove rows which contain ratings of 3 (Neutral and not included in our analysis.)\n",
    "data = data[data.stars != 3]\n",
    "print(data.head())\n",
    "data['stars'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    65606\n",
       "0    21906\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.stars <=2, 'stars'] = 0\n",
    "data.loc[data.stars >=4, 'stars'] = 1\n",
    "        \n",
    "data['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text\n",
      "1      0  The food was perfectly adequate, nothing speci...\n",
      "2      0  Our server, Cookie, has been fabulous both tim...\n",
      "3      0  DO NOT go here, I made reservations for a holi...\n",
      "4      1  My husband and I stopped in for a foot rub and...\n",
      "6      1  Always great, fresh food. Quick service. \\n\\nT...\n"
     ]
    }
   ],
   "source": [
    "#make sure the reviews were labelled correctly (can compare to the previous header)\n",
    "print data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split dataset into train/test sets\n",
    "train_data = data.sample(frac=0.7,random_state=200)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "train_data.to_csv('train.csv', index=False, sep=',', encoding='utf-8')\n",
    "test_data.to_csv('test.csv', index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples are: 61258\n",
      "The number of testing samples are: 26254 \n",
      "\n",
      "   stars                                               text\n",
      "0      1  Best place ever!  I was so scared and they mad...\n",
      "1      1  I've had the last week off before starting my ...\n",
      "   stars                                               text\n",
      "0      1  My husband and I stopped in for a foot rub and...\n",
      "1      1  Always great, fresh food. Quick service. \\n\\nT...\n"
     ]
    }
   ],
   "source": [
    "#load train/test sets\n",
    "train = pd.read_csv('train.csv', index_col=False)\n",
    "test = pd.read_csv('test.csv', index_col=False)\n",
    "\n",
    "print (\"The number of training samples are: %r\") % (len(train))\n",
    "print (\"The number of testing samples are: %r \\n\") % (len(test))\n",
    "\n",
    "#make sure the train/test tests are formatted correctly.\n",
    "print train.iloc[:2]\n",
    "print test.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing & Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the Amazon reviews...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list. Word2VecUtility is a text processing function imported from another file. \n",
    "\n",
    "print(\"Cleaning and parsing the Amazon reviews...\\n\")\n",
    "for i in range( 0, len(train[\"text\"])):\n",
    "    clean_train_reviews.append(\" \".join(Word2VecUtility.review_to_wordlist(train[\"text\"][i], True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ****** Create a bag of words from the training set\n",
    "#\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                         tokenizer = None,    \\\n",
    "                         preprocessor = None, \\\n",
    "                         stop_words = None,   \\\n",
    "                         max_features = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SVM (this may take a while)...\n"
     ]
    }
   ],
   "source": [
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of\n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an\n",
    "# array\n",
    "np.asarray(train_data_features)\n",
    "\n",
    "# ******* Train an SVM using the bag of words\n",
    "#\n",
    "print(\"Training the SVM (this may take a while)...\")\n",
    "\n",
    "# Fit the SVM to the training set, using the bag of words as\n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# Initialize an SVM classifier with chosen parameters.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SVM = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "SVM = SVM.fit( train_data_features, train[\"stars\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Predicting test labels...\n",
      "\n",
      "Wrote results to Bag_of_Words_model.csv\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list and append the clean reviews one by one\n",
    "clean_test_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,len(test[\"text\"])):\n",
    "    clean_test_reviews.append(\" \".join(Word2VecUtility.review_to_wordlist(test[\"text\"][i], True)))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "np.asarray(test_data_features)\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "print(\"Predicting test labels...\\n\")\n",
    "result = SVM.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"stars\"], \"Sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join('Bag_of_Words_modelSVM100k.csv'), index=False, quoting=3)\n",
    "print(\"Wrote results to Bag_of_Words_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933305401082\n"
     ]
    }
   ],
   "source": [
    "accuraccy = np.mean(result == test['stars'])  \n",
    "\n",
    "print(accuraccy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text\n",
      "0      1  I just got my reading glasses back and have no...\n",
      "1      1  Absolutely the \"Best of Phoenix\". Caring compa...\n",
      "2      1  I've been coming here for about 4 years now. I...\n",
      "3      1  The food was hot and tasty. The garlic knots a...\n",
      "4      1  This place is delicious!! I love the salmon an...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    100000\n",
       "0    100000\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in the new data\n",
    "df = pd.read_csv('Yelp_Evenly_Sampled.csv', index_col=False)\n",
    "print data.iloc[:5]\n",
    "data['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split dataset into train/test sets\n",
    "#changed names so that we don't contaminate data \n",
    "train_balanced_data = df.sample(frac=0.7,random_state=200)\n",
    "test_balanced_data = df.drop(train_balanced_data.index)\n",
    "\n",
    "train_balanced_data.to_csv('train_balanced.csv', index=False, sep=',', encoding='utf-8')\n",
    "test_balanced_data.to_csv('test_balanced.csv', index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples are: 140000\n",
      "The number of testing samples are: 60000 \n",
      "\n",
      "   stars                                               text\n",
      "0      0  I agree with most of the reviews on the site. ...\n",
      "1      0  The thing I hate about them is that I can neve...\n",
      "   stars                                               text\n",
      "0      1  I just got my reading glasses back and have no...\n",
      "1      1  I've been coming here for about 4 years now. I...\n"
     ]
    }
   ],
   "source": [
    "#load train/test sets\n",
    "train1 = pd.read_csv('train_balanced.csv', index_col=False)\n",
    "test1 = pd.read_csv('test_balanced.csv', index_col=False)\n",
    "\n",
    "print (\"The number of training samples are: %r\") % (len(train1))\n",
    "print (\"The number of testing samples are: %r \\n\") % (len(test1))\n",
    "\n",
    "#make sure the train/test tests are formatted correctly.\n",
    "print train1.iloc[:2]\n",
    "print test1.iloc[:2]\n",
    "#print(train1['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the Amazon reviews...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Users\\James\\Anaconda2\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "C:\\Users\\James\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.azcentral.com/news/articles/2010/12/20/20101220chandler-movers-sued-judgment.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews1 = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list. Word2VecUtility is a text processing function imported from another file. \n",
    "\n",
    "print(\"Cleaning and parsing the Amazon reviews...\\n\")\n",
    "for i in range( 0, len(train1[\"text\"])):\n",
    "    clean_train_reviews1.append(\" \".join(Word2VecUtility.review_to_wordlist(train1[\"text\"][i], True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ****** Create a bag of words from the training set\n",
    "#\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer1 = CountVectorizer(analyzer = \"word\",   \\\n",
    "                         tokenizer = None,    \\\n",
    "                         preprocessor = None, \\\n",
    "                         stop_words = None,   \\\n",
    "                         max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ****** Create a bag of words from the training set\n",
    "#\n",
    "print(\"Creating the bag of words...\\n\")\n",
    "\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer1 = CountVectorizer(analyzer = \"word\",   \\\n",
    "                         tokenizer = None,    \\\n",
    "                         preprocessor = None, \\\n",
    "                         stop_words = None,   \\\n",
    "                         max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SVM (this may take a while)...\n"
     ]
    }
   ],
   "source": [
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of\n",
    "# strings.\n",
    "train_data_features1 = vectorizer1.fit_transform(clean_train_reviews1)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an\n",
    "# array\n",
    "np.asarray(train_data_features1)\n",
    "\n",
    "# ******* Train an SVM using the bag of words\n",
    "#\n",
    "print(\"Training the SVM (this may take a while)...\")\n",
    "\n",
    "# Fit the SVM to the training set, using the bag of words as\n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# Initialize an SVM classifier with chosen parameters.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "SVM1 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "SVM1 = SVM1.fit( train_data_features1, train1[\"stars\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Predicting test labels...\n",
      "\n",
      "Wrote results to Bag_of_Words_model.csv\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list and append the clean reviews one by one\n",
    "clean_test_reviews1 = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,len(test1[\"text\"])):\n",
    "    clean_test_reviews1.append(\" \".join(Word2VecUtility.review_to_wordlist(test1[\"text\"][i], True)))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features1 = vectorizer1.transform(clean_test_reviews1)\n",
    "np.asarray(test_data_features1)\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "print(\"Predicting test labels...\\n\")\n",
    "result1 = SVM1.predict(test_data_features1)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output1 = pd.DataFrame( data={\"id\":test1[\"stars\"], \"Sentiment\":result1} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output1.to_csv(os.path.join('Bag_of_Words_Model_BalancedSVM82k.csv'), index=False, quoting=3)\n",
    "print(\"Wrote results to Bag_of_Words_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.927716666667\n"
     ]
    }
   ],
   "source": [
    "accuraccy1 = np.mean(result1 == test1['stars'])  \n",
    "\n",
    "print(accuraccy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
