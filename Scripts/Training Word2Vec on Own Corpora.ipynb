{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from Word2VecUtility3 import Word2VecUtility3\n",
    "from __future__ import division\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 164000 entries, 0 to 163999\n",
      "Data columns (total 2 columns):\n",
      "Score    164000 non-null int64\n",
      "Text     164000 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.5+ MB\n",
      "None\n",
      "   Score                                               Text\n",
      "0      1  I like this brand. I didn't realize I was orde...\n",
      "1      1  Being my wife is a licensed cosmetologist and ...\n",
      "2      1  If you are looking for an upgrade from the sta...\n",
      "3      1  I am so allergic to too many artificial sweete...\n",
      "4      1  I have not been able to find this locally and ...\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('AFF_Evenly_Sampled.csv', encoding='utf-8')\n",
    "print(data.info())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Train Word2vec Model on Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the reviews...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.amazon.com/gp/product/B004HYXZB6/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.amazon.com/Greenies-Dental-Chews-ounce-Teenie/dp/B001CWEC2W/ref=sr_1_1?ie=UTF8&s=home-garden&qid=1266273814&sr=8-1\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'............  ..  ......  ........  .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\james\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.doyourownpestcontrol.com/SPEC/pick-gophertrap.htm\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "review_sents = []\n",
    "print(\"Cleaning and parsing the reviews...\\n\")\n",
    "for i in range( 0, len(data[\"Text\"])):\n",
    "    # sent_reviews += Word2VecUtility.review_to_sentences(data[\"text\"][i], tokenizer)\n",
    "    review_sents += Word2VecUtility3.review_to_sentences(data.iloc[i][\"Text\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843499\n",
      "[['i', 'like', 'this', 'brand'], ['i', 'didn', 't', 'realize', 'i', 'was', 'ordering', 'extra', 'bold'], ['to', 'me', 'these', 't', 'cups', 'are', 'bitter'], ['since', 'i', 'can', 't', 'control', 'the', 'strength', 'i', 'make', 'a', 'small', 'cup', 'and', 'toss'], ['then', 'i', 'make', 'a', 'large', 'cup', 'to', 'drink']]\n"
     ]
    }
   ],
   "source": [
    "# review_sents = pickle.load(open('review_sents_1859888.pkl', 'rb'))\n",
    "print(len(review_sents))\n",
    "print(review_sents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(review_sents, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"word2vecAFF\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"word2vecAFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.57762066e-02,  -8.39834809e-02,   8.17066617e-03,\n",
       "         1.04141282e-02,  -2.66474462e-03,  -2.27323528e-02,\n",
       "        -2.14354713e-02,  -3.53646791e-03,  -4.52830121e-02,\n",
       "         9.52791423e-02,  -1.06150575e-01,   7.16838837e-02,\n",
       "        -5.28025031e-02,   9.54057649e-02,  -9.97108966e-03,\n",
       "        -4.38281223e-02,  -2.80837640e-02,   8.40951204e-02,\n",
       "         2.82229371e-02,  -4.01410945e-02,   1.00977402e-02,\n",
       "        -4.42988314e-02,   4.73538451e-02,  -1.36963530e-02,\n",
       "         4.46166731e-02,   6.49208650e-02,  -3.77086774e-02,\n",
       "         7.24832341e-02,   4.36785296e-02,  -9.58878919e-02,\n",
       "         4.29454399e-03,   1.80076547e-02,  -9.95138753e-03,\n",
       "         5.35431653e-02,  -5.14382089e-04,  -2.79666297e-03,\n",
       "         8.21596086e-02,  -2.82009020e-02,  -1.92336785e-03,\n",
       "        -3.73922028e-02,   4.43739966e-02,   2.77878605e-02,\n",
       "        -6.89840019e-02,  -5.17202402e-03,   2.13419367e-02,\n",
       "        -4.61289883e-02,   9.91263986e-02,  -8.44704267e-03,\n",
       "        -5.81067912e-02,   4.83732633e-02,  -2.75381971e-02,\n",
       "         1.51247308e-01,   8.36237222e-02,  -4.79219779e-02,\n",
       "        -4.97862212e-02,  -1.12458996e-01,  -4.58649360e-02,\n",
       "        -3.21446359e-02,  -1.56853162e-02,   1.87823698e-02,\n",
       "        -1.41188875e-02,   8.18178654e-02,  -1.89131856e-01,\n",
       "        -3.35868560e-02,  -5.36019579e-02,   3.53974663e-02,\n",
       "        -8.25699642e-02,  -1.27007976e-01,  -2.39129011e-02,\n",
       "         5.27226850e-02,   2.07508147e-01,   5.39301783e-02,\n",
       "         6.50963783e-02,  -5.18427156e-02,   6.29597271e-05,\n",
       "        -2.18794346e-02,  -2.20263004e-02,  -7.93317705e-02,\n",
       "        -1.44007057e-02,  -2.94603296e-02,  -5.16993180e-02,\n",
       "        -4.78093997e-02,   1.10019960e-01,   5.24772406e-02,\n",
       "         3.02056298e-02,  -9.08799917e-02,  -1.84264556e-02,\n",
       "        -8.68774056e-02,   7.99965933e-02,  -3.86912674e-02,\n",
       "         5.21550588e-02,   3.95766273e-02,   9.31272209e-02,\n",
       "         2.73047034e-02,  -2.55023763e-02,  -1.30231336e-01,\n",
       "         5.89084402e-02,   3.90300490e-02,  -6.51065214e-03,\n",
       "         1.71623919e-02,  -1.35626141e-02,  -2.91636400e-02,\n",
       "        -1.42512461e-02,   5.56192696e-02,   9.59442481e-02,\n",
       "        -4.33034189e-02,  -7.11077009e-04,  -1.05835879e-02,\n",
       "         1.00824364e-01,  -2.16243174e-02,  -1.84558891e-02,\n",
       "         4.45085093e-02,   4.50447248e-03,   6.24646433e-02,\n",
       "         9.70828310e-02,  -1.15413152e-01,  -1.52434213e-02,\n",
       "         1.37445526e-02,   1.47913955e-02,   2.77017877e-02,\n",
       "        -4.99017946e-02,   1.34243499e-02,  -3.48150395e-02,\n",
       "        -2.28894055e-02,   9.28103253e-02,   8.48500878e-02,\n",
       "         1.10652102e-02,  -2.83379573e-02,  -1.36172101e-01,\n",
       "        -2.35758051e-02,  -8.31586123e-02,  -4.74997722e-02,\n",
       "        -5.00314906e-02,   1.77174881e-02,   2.96098664e-02,\n",
       "         2.02072202e-03,  -4.93805259e-02,  -2.68362015e-02,\n",
       "         2.27484293e-02,   4.21325527e-02,   3.92372869e-02,\n",
       "        -4.74806838e-02,   2.89347135e-02,   7.85915647e-03,\n",
       "        -2.69666184e-02,  -4.24083322e-02,   1.41301323e-02,\n",
       "         5.32771982e-02,   6.83314204e-02,   4.51771952e-02,\n",
       "         3.64610255e-02,  -5.33779897e-02,  -3.50639932e-02,\n",
       "        -1.19385002e-02,   4.78318445e-02,  -1.37348156e-02,\n",
       "         1.78557597e-02,  -5.22699729e-02,   7.38191232e-03,\n",
       "        -8.63583907e-02,   3.87915261e-02,  -2.50539761e-02,\n",
       "         3.42947319e-02,  -2.32093800e-02,  -5.71670979e-02,\n",
       "         9.42520797e-03,   5.06042726e-02,  -2.94593424e-02,\n",
       "        -8.06411728e-02,   3.55787612e-02,   5.18516414e-02,\n",
       "        -5.69183491e-02,  -4.22662608e-02,   9.49912742e-02,\n",
       "        -3.13360915e-02,  -5.89705817e-02,  -1.05070639e-02,\n",
       "        -1.16745790e-03,   4.78862189e-02,   4.53303196e-02,\n",
       "        -2.08835374e-03,   5.89626618e-02,  -5.49797304e-02,\n",
       "         1.02352444e-02,   3.59403491e-02,   1.10668063e-01,\n",
       "        -7.30594099e-02,   4.05171998e-02,  -2.59750411e-02,\n",
       "         1.06299808e-02,   7.90239051e-02,   1.06928439e-03,\n",
       "         5.13639376e-02,   4.02399264e-02,   1.53326122e-02,\n",
       "         8.48035514e-02,  -3.67484689e-02,  -6.24338053e-02,\n",
       "         6.02050833e-02,  -7.99212903e-02,   1.07271977e-01,\n",
       "         1.47388391e-02,   5.57594113e-02,   5.00605218e-02,\n",
       "        -4.64804880e-02,  -3.79550941e-02,  -6.14591911e-02,\n",
       "         1.58691444e-02,  -5.15011884e-02,  -4.90401126e-02,\n",
       "        -1.40005676e-02,  -1.29862502e-03,   2.20908280e-02,\n",
       "         8.77201408e-02,  -1.79765001e-02,  -2.19715293e-02,\n",
       "        -4.53844257e-02,  -2.51497757e-02,   6.03256300e-02,\n",
       "        -7.88219869e-02,   8.50227196e-03,   7.28999153e-02,\n",
       "         5.05376980e-02,   4.59207557e-02,  -2.27733850e-02,\n",
       "        -7.95204863e-02,  -2.03629918e-02,  -1.08008981e-01,\n",
       "         5.19316383e-02,  -3.59709635e-02,   3.48472148e-02,\n",
       "         3.76531631e-02,   1.89509224e-02,  -2.78266836e-02,\n",
       "         7.40859210e-02,   2.96151470e-02,   1.23593910e-02,\n",
       "         3.05576120e-02,   1.32374212e-01,   8.21228325e-02,\n",
       "        -2.51865387e-02,   1.46262586e-01,   3.03510763e-02,\n",
       "        -1.25504583e-01,   1.44679891e-02,  -1.17918374e-02,\n",
       "         2.72890031e-02,   2.44569089e-02,   8.84510204e-03,\n",
       "        -1.07499966e-02,   9.81390327e-02,   8.43943805e-02,\n",
       "         1.30884452e-02,   3.73942852e-02,  -3.69163090e-03,\n",
       "         9.10587981e-02,   9.91654620e-02,   4.37207296e-02,\n",
       "         2.31257193e-02,   3.46430093e-02,   1.00930734e-02,\n",
       "        -9.56240520e-02,   7.26164952e-02,   9.41005126e-02,\n",
       "         1.03410846e-02,   6.16976582e-02,   4.67582606e-03,\n",
       "         4.25198898e-02,   4.61903140e-02,  -1.00457452e-01,\n",
       "        -1.51840542e-02,  -2.27380032e-03,   8.06685686e-02,\n",
       "         1.51143949e-02,  -1.14772089e-01,   1.09415654e-04,\n",
       "         3.77755687e-02,  -3.90831232e-02,   1.34298345e-02,\n",
       "         7.89679438e-02,  -1.29001419e-04,  -1.25051260e-01,\n",
       "         1.33858006e-02,   1.14412628e-01,   9.86911450e-03,\n",
       "         6.48806766e-02,  -4.02186774e-02,   6.40692860e-02,\n",
       "         1.70664955e-02,  -3.23220342e-02,  -8.31132531e-02,\n",
       "        -1.03698131e-02,   5.18921716e-03,   5.57416342e-02,\n",
       "         1.72101967e-02,   9.81264710e-02,   9.32752341e-03,\n",
       "         6.92292750e-02,  -8.81682932e-02,   4.94800461e-03], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['computer']  # numpy vector of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Creating Word Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import gensim\n",
    "from Word2VecUtility3 import Word2VecUtility3\n",
    "\n",
    "BASE_DIR = 'C:/Users/James/Desktop/NLP Summer/'\n",
    "#EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "#GLOVE_DIR = BASE_DIR + '/glove.42B.300d/'\n",
    "TRAIN_DATA_FILE = BASE_DIR + '/AFF/AFF_Evenly_Sampled.csv'\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164000 texts in train.csv\n",
      "Found 69963 unique tokens\n",
      "Shape of data tensor: (164000, 1000)\n",
      "Shape of label tensor: (164000,)\n"
     ]
    }
   ],
   "source": [
    "#create arrays for text data and labels\n",
    "\n",
    "texts = [] \n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts.append(text_to_wordlist(values[1]))\n",
    "        labels.append(int(values[0]))\n",
    "print('Found %s texts in train.csv' % len(texts))\n",
    "\n",
    "#store and load using pickle\n",
    "pickle.dump((texts, labels), open(\"texts_labels_index.pkl\", 'wb'))\n",
    "(texts, labels) = pickle.load(open(\"texts_labels_index.pkl\", 'rb'))\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8839, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#index2word_set = set(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 61133\n"
     ]
    }
   ],
   "source": [
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix_w2v = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in model.wv.vocab:\n",
    "        embedding_matrix_w2v[i] = model.wv.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix_w2v, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer_w2v = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix_w2v],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer_w2v(sequence_input)\n",
    "x = lstm_layer(embedded_sequences)\n",
    "x = Dropout(rate_drop_dense)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(num_dense, activation=act)(x)\n",
    "x = Dropout(rate_drop_dense)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         20989200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 233)               497688    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 233)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 233)               932       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 124)               29016     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 124)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 124)               496       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 125       \n",
      "=================================================================\n",
      "Total params: 21,517,457\n",
      "Trainable params: 527,543\n",
      "Non-trainable params: 20,989,914\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## train the model\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120266 samples, validate on 10934 samples\n",
      "Epoch 1/2\n",
      "1027s - loss: 0.4320 - acc: 0.8006 - val_loss: 0.3097 - val_acc: 0.8748\n",
      "Epoch 2/2\n",
      "1054s - loss: 0.4244 - acc: 0.7990 - val_loss: 0.3789 - val_acc: 0.8349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e028e1c18>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=128, verbose=2, validation_split =1/12.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 37.19%\n",
      "Accuracy: 84.26%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Test Score: %.2f%%\" % (scores[0]*100))\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1140s - loss: 0.3187 - acc: 0.8646\n",
      "Epoch 2/2\n",
      "1111s - loss: 0.2692 - acc: 0.8893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e3b4baa58>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 23.03%\n",
      "Accuracy: 90.47%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Test Score: %.2f%%\" % (scores[0]*100))\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
